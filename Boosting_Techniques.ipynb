{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Boosting Techniques**"
      ],
      "metadata": {
        "id": "28ZgHLT1EczF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Boosting Techniques Question/Answers:"
      ],
      "metadata": {
        "id": "6ysRUiJvEp4J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Boosting in Machine Learning? Explain how it improves weak\n",
        "learners.\n",
        "    - Boosting is an ***ensemble learning technique*** that combines multiple ***weak learners*** (models that perform slightly better than random guessing) to form a ***strong predictive model***.\n",
        "\n",
        "     Boosting works by:\n",
        "\n",
        "* Training models *sequentially*\n",
        "* Giving *more importance to misclassified samples*\n",
        "* Each new model focuses on correcting the errors made by previous models\n",
        "\n",
        "  By iteratively reducing errors, boosting improves overall accuracy and reduces bias, resulting in a powerful model.\n"
      ],
      "metadata": {
        "id": "pVFbwi_AExw3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the difference between AdaBoost and Gradient Boosting in terms\n",
        "of how models are trained?\n",
        "\n",
        "| Aspect            | AdaBoost                                  | Gradient Boosting              |\n",
        "| ----------------- | ----------------------------------------- | ------------------------------ |\n",
        "| Error handling    | Increases weight of misclassified samples | Fits models to residual errors |\n",
        "| Loss function     | Exponential loss                          | Any differentiable loss        |\n",
        "| Flexibility       | Less flexible                             | Highly flexible                |\n",
        "| Noise sensitivity | Sensitive to outliers                     | More robust                    |\n",
        "\n"
      ],
      "metadata": {
        "id": "_CXUVHm4FcCD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. How does regularization help in XGBoost?\n",
        "\n",
        "    - Regularization in XGBoost helps *prevent overfitting* by penalizing model complexity.\n",
        "\n",
        "   * ***L1 regularization (alpha)*** reduces irrelevant features\n",
        "   * ***L2 regularization (lambda)*** controls large weights\n",
        "   * Penalizes deep trees and unnecessary splits\n",
        "\n",
        "This leads to ***simpler trees***, better generalization, and improved performance on unseen data."
      ],
      "metadata": {
        "id": "kZOXBDVUFxFQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Why is CatBoost considered efficient for handling categorical data?\n",
        "    - **CatBoost is efficient because:**\n",
        "\n",
        "* It ***handles categorical features directly*** (no one-hot encoding needed)\n",
        "* Uses ***ordered target encoding*** to prevent data leakage\n",
        "* Reduces preprocessing effort\n",
        "* Improves accuracy and training speed\n",
        "\n",
        "Thus, CatBoost performs especially well on datasets with many categorical features."
      ],
      "metadata": {
        "id": "AqjgzfegGTXx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What are some real-world applications where boosting techniques are\n",
        "preferred over bagging methods?\n",
        "\n",
        "     Datasets:\n",
        "\n",
        "     ‚óè Use sklearn.datasets.load_breast_cancer() for classification tasks.\n",
        "\n",
        "     ‚óè Use sklearn.datasets.fetch_california_housing() for regression\n",
        "      tasks.\n",
        "       \n"
      ],
      "metadata": {
        "id": "EPqVw7udGp7K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Bagging model (Random Forest)\n",
        "bagging_model = RandomForestClassifier(random_state=42)\n",
        "bagging_model.fit(X_train, y_train)\n",
        "bagging_pred = bagging_model.predict(X_test)\n",
        "\n",
        "# Boosting model (Gradient Boosting)\n",
        "boosting_model = GradientBoostingClassifier(random_state=42)\n",
        "boosting_model.fit(X_train, y_train)\n",
        "boosting_pred = boosting_model.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "print(\"Bagging Accuracy (Random Forest):\",\n",
        "      accuracy_score(y_test, bagging_pred))\n",
        "print(\"Boosting Accuracy (Gradient Boosting):\",\n",
        "      accuracy_score(y_test, boosting_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1F1a1naNll0",
        "outputId": "3030c2e9-1451-45cc-f459-87cdb118250d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Accuracy (Random Forest): 0.9649122807017544\n",
            "Boosting Accuracy (Gradient Boosting): 0.956140350877193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîπ Regression: California Housing (Boosting vs Bagging)"
      ],
      "metadata": {
        "id": "mxxqeLImN0YG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "\n",
        "# Load dataset\n",
        "X, y = fetch_california_housing(return_X_y=True)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Bagging model\n",
        "bagging_reg = RandomForestRegressor(random_state=42)\n",
        "bagging_reg.fit(X_train, y_train)\n",
        "bagging_pred = bagging_reg.predict(X_test)\n",
        "\n",
        "# Boosting model\n",
        "boosting_reg = GradientBoostingRegressor(random_state=42)\n",
        "boosting_reg.fit(X_train, y_train)\n",
        "boosting_pred = boosting_reg.predict(X_test)\n",
        "\n",
        "# Mean Squared Error\n",
        "print(\"Bagging MSE:\", mean_squared_error(y_test, bagging_pred))\n",
        "print(\"Boosting MSE:\", mean_squared_error(y_test, boosting_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnXlhjRrN-19",
        "outputId": "ae193df2-26f3-49b8-db54-98acaa682b97"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging MSE: 0.2553684927247781\n",
            "Boosting MSE: 0.2939973248643864\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Write a Python program to:\n",
        "\n",
        "   ‚óè Train an AdaBoost Classifier on the Breast Cancer dataset\n",
        "   \n",
        "   ‚óè Print the model accuracy"
      ],
      "metadata": {
        "id": "4tCwkprqHuNr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UAFm78cEZ7R",
        "outputId": "d5b8822a-ecd9-488c-cda1-c3f0c540f313"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9736842105263158\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train AdaBoost model\n",
        "model = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict & accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Write a Python program to:\n",
        "\n",
        "   ‚óè Train a Gradient Boosting Regressor on the California Housing dataset\n",
        "\n",
        "   ‚óè Evaluate performance using R-squared score"
      ],
      "metadata": {
        "id": "53WoVK1AICxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train model\n",
        "model = GradientBoostingRegressor(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict & R2 score\n",
        "y_pred = model.predict(X_test)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"R-squared Score:\", r2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mu2LRloLIOM4",
        "outputId": "41fb95bf-fc6e-47b8-a756-c8366c5683b4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R-squared Score: 0.7756446042829697\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Write a Python program to:\n",
        "\n",
        "   ‚óè Train an XGBoost Classifier on the Breast Cancer dataset\n",
        "\n",
        "   ‚óè Tune the learning rate using GridSearchCV\n",
        "\n",
        "   ‚óè Print the best parameters and accuracy\n"
      ],
      "metadata": {
        "id": "vQTC2ET9IVv6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import xgboost as xgb\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Define XGBoost Classifier\n",
        "model = xgb.XGBClassifier(\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='logloss',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Parameter grid for tuning learning rate\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2]\n",
        "}\n",
        "\n",
        "# GridSearchCV\n",
        "griid = GridSearchCV(\n",
        "    estimator=model,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy'\n",
        ")\n",
        "\n",
        "# Train model\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = grid.best_estimator_.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Test Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIEltBK2P-Ya",
        "outputId": "a1c6d68f-f3cc-492e-ec1e-9909fa68bde6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [16:32:42] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [16:32:42] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [16:32:43] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [16:32:43] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [16:32:43] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [16:32:43] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [16:32:43] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [16:32:43] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [16:32:43] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [16:32:43] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'learning_rate': 0.1}\n",
            "Test Accuracy: 0.956140350877193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Write a Python program to:\n",
        "\n",
        "   ‚óè Train a CatBoost Classifier\n",
        "\n",
        "   ‚óè Plot the confusion matrix using seaborn"
      ],
      "metadata": {
        "id": "K90_yl3zIrHt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train CatBoost Classifier\n",
        "model = CatBoostClassifier(\n",
        "    iterations=100,\n",
        "    learning_rate=0.1,\n",
        "    depth=6,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot Confusion Matrix\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix - CatBoost Classifier\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Lh3WXcJ-RA0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. You're working for a FinTech company trying to predict loan default using\n",
        "customer demographics and transaction behavior.\n",
        "The dataset is imbalanced, contains missing values, and has both numeric and\n",
        "categorical features.\n",
        "\n",
        "     Describe your step-by-step data science pipeline using boosting techniques:\n",
        "\n",
        "     ‚óè Data preprocessing & handling missing/categorical values\n",
        "\n",
        "     ‚óè Choice between AdaBoost, XGBoost, or CatBoost\n",
        "\n",
        "     ‚óè Hyperparameter tuning strategy\n",
        "\n",
        "     ‚óè Evaluation metrics you'd choose and why\n",
        "\n",
        "     ‚óè How the business would benefit from your model\n",
        "\n",
        "     ### *1. Data Preprocessing & Handling Missing / Categorical Values*\n",
        "\n",
        "*a) Initial Data Understanding*\n",
        "\n",
        "* Check class imbalance (default vs non-default).\n",
        "* Identify numeric and categorical features.\n",
        "* Analyze missing value patterns.\n",
        "\n",
        "*b) Handling Missing Values*\n",
        "\n",
        "* *Numerical features*:\n",
        "\n",
        "  * Use median/mean imputation or model-based imputation.\n",
        "* *Categorical features*:\n",
        "\n",
        "  * Replace missing values with \"Unknown\" or most frequent category.\n",
        "* Some boosting models (like XGBoost & CatBoost) can handle missing values internally.\n",
        "\n",
        "*c) Encoding Categorical Variables*\n",
        "\n",
        "* *One-Hot Encoding* ‚Üí if categories are low.\n",
        "* *Target / Ordinal Encoding* ‚Üí if categories are high.\n",
        "* *CatBoost* can directly handle categorical features without encoding.\n",
        "*d) Handling Imbalanced Dataset*\n",
        "\n",
        "* Use:\n",
        "\n",
        "  * Class weights (scale_pos_weight)\n",
        "  * SMOTE or undersampling (if needed)\n",
        "* Prefer *cost-sensitive learning* over heavy resampling.\n",
        "\n",
        "---\n",
        "\n",
        "### *2. Choice Between AdaBoost, XGBoost, or CatBoost*\n",
        "\n",
        "| Model                      | Reason                                                                             |\n",
        "| -------------------------- | ---------------------------------------------------------------------------------- |\n",
        "| *AdaBoost*               | Simple, but sensitive to noise & missing values                                    |\n",
        "| *XGBoost*                | High performance, handles missing values, scalable                                 |\n",
        "| *CatBoost (Best choice)* | Handles categorical data automatically, robust to imbalance, minimal preprocessing |\n",
        "\n",
        "‚úÖ *Final Choice: CatBoost or XGBoost*\n",
        "\n",
        "* *CatBoost* ‚Üí Best when many categorical features exist\n",
        "* *XGBoost* ‚Üí Best when features are mostly numeric and dataset is large\n",
        "\n",
        "---\n",
        "\n",
        "### *3. Hyperparameter Tuning Strategy*\n",
        "\n",
        "*a) Baseline Model*\n",
        "* Train with default parameters to get a benchmark.\n",
        "\n",
        "*b) Important Hyperparameters*\n",
        "\n",
        "* learning_rate\n",
        "* n_estimators\n",
        "* max_depth\n",
        "* subsample\n",
        "* colsample_bytree\n",
        "* scale_pos_weight (for imbalance)\n",
        "\n",
        "*c) Tuning Methods*\n",
        "\n",
        "* Grid Search (small datasets)\n",
        "* Random Search (large datasets)\n",
        "* Bayesian Optimization (efficient & faster)\n",
        "*d) Cross-Validation*\n",
        "\n",
        "* Use *Stratified K-Fold CV* to maintain class balance.\n",
        "\n",
        "---\n",
        "\n",
        "### *4. Evaluation Metrics & Why*\n",
        "\n",
        "Since the dataset is *imbalanced*, accuracy alone is misleading.\n",
        "\n",
        "‚úÖ *Preferred Metrics:*\n",
        "\n",
        "| Metric        | Reason                                                  |\n",
        "| ------------- | ------------------------------------------------------- |\n",
        "| *ROC-AUC*   | Measures overall discrimination ability                 |\n",
        "| *Precision* | Controls false positives (important for loan approvals) |\n",
        "| *Recall*    | Captures defaulters (reduces financial risk)            |\n",
        "| *F1-Score*  | Balance between precision & recall                      |\n",
        "| *PR-AUC*    | Better for imbalanced datasets                          |\n",
        "\n",
        "üìå *Business Priority*:\n",
        "\n",
        "* Higher *Recall for defaulters* ‚Üí avoid risky customers\n",
        "* Balanced with *Precision* to avoid rejecting good customers\n",
        "\n",
        "---\n",
        "\n",
        "### *5. How the Business Benefits from This Model*\n",
        "\n",
        "\n",
        "* ‚úÖ *Reduced Loan Defaults* ‚Üí lower financial loss\n",
        "* ‚úÖ *Better Credit Risk Assessment*\n",
        "* ‚úÖ *Automated & Faster Loan Decisions*\n",
        "* ‚úÖ *Improved Profitability*\n",
        "* ‚úÖ *Explainability* using feature importance & SHAP values\n",
        "* ‚úÖ *Personalized Interest Rates & Credit Limits*\n"
      ],
      "metadata": {
        "id": "A2kJV5GLJuVb"
      }
    }
  ]
}